# -*- coding: utf-8 -*-
"""Home Equity Coin - Regression Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UeBHdL_o2hY3sOSFHyqhNYnYI0r8UmOn

#Step 1: Data Filtration

## Loading Packages and Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

!gdown --id "1RJSUB-Wf4WUg1AXMtW2X2wD9B2SIHYT8"

orig_data = pd.read_csv("2019_assessments_fips_113135_gwinnett.csv")

data = orig_data.copy()

data.columns

data.head()

"""## Data Segregation"""

dump_df = pd.DataFrame()

# "apn" may also not be the potential feature for the regression model, we put it into the "dump_df"
dump_df["apn"] = data["apn"]
dump_df.head()

# Dropping the column "apn" from "data"
data = data.drop(columns="apn")
data.head()

# Since we have divided the address to city, house and siffix and zip code
# We do not need the full address; Keeping it in dump_df

dump_df["address.full"] = data["address.full"]
dump_df.head()

data = data.drop(columns="address.full")
data.head()

new_data = pd.DataFrame()

print(data["address.city"].unique())

# We need the city. We keep the column and convert all the values into lower case
# We add them to new_data
data["address.city"] = data["address.city"]
# Whats the point of the line above this comment? - Joed
new_data["city"] = data["address.city"]
new_data.head()

data = data.drop(columns="address.city")
data.head()

# These are required: address.state,	address.house,	address.street, address.streetSuffix,	address.unit,	address.zip,	address.zip4
# Adding them to new_data
temp_df = data.iloc[:,:7]
temp_cols = list(temp_df.columns)
print(temp_cols)
new_cols = list(map(lambda x: x.split('.')[1], temp_cols))
temp_cols_dict = dict(zip(temp_cols, new_cols))
print(temp_cols_dict)

# We keep the State, House Number, Street Name, suffix, unit and zip code
# State is only one, but can be anything -> We might want to encode all the 50 state of USA
temp_df = temp_df.rename(columns=temp_cols_dict)
new_data = pd.concat([new_data, temp_df], axis=1)
new_data.head()

data = data.drop(columns=temp_cols)
data.head()

# We don't need "fips" since it has only single value, adding to dump_df
print(data["fips"].unique())

dump_df["fips"] = data["fips"]
dump_df.head()

data = data.drop(columns="fips")
data.head()

print(data["county"].unique())

#Only "Gwinnett County" -> Can be dropped
dump_df["county"] = data["county"]
dump_df.head()

data = data.drop(columns="county")
data.head()

print(data["state"].unique())

# Already loaded
dump_df["state"] = data["state"]
dump_df.head()

data = data.drop(columns="state")
data.head()

# We keep latitude, longitude
temp_df = data.iloc[:,:2]
temp_cols = list(temp_df.columns)
new_cols = ["latitude","longitude"]
temp_cols_dict = dict(zip(temp_cols, new_cols))
print(temp_cols_dict)

temp_df = temp_df.rename(columns=temp_cols_dict)
new_data = pd.concat([new_data, temp_df], axis=1)
new_data.head()

data = data.drop(columns=temp_cols)
data.head()

print(data["censusTract"].unique())

# What is censusTract? Do we need it? Lets keep it in confusion_df and remove from "data"
confusion_df = pd.DataFrame()
confusion_df["censusTract"] = data["censusTract"]
confusion_df.head()

data = data.drop(columns="censusTract")
data.head()

print(data["taxID"].unique())

# We don't have taxID, since no value, so add to dump_df
dump_df["taxID"] = data["taxID"]
dump_df.head()

data = data.drop(columns=["taxID"])
data.head()

# How are we going to process the taxAmount and taxYear, are they the direct features for Regression Model? For now lets keep in confusion_df
confusion_df = pd.concat([confusion_df, data.iloc[:,:2]], axis=1)
confusion_df.head()

data = data.drop(columns=["taxAmount", "taxYear"])
data.head()

# What is totalValue? For now lets keep it in "confusion_df"
confusion_df["totalValue"] = data["totalValue"]
confusion_df.head()

data.drop(columns="totalValue", inplace=True)
data.head()

print(data.marketValueYear.unique())

# Since there is only 2019, we may not include this, but still the data could be of any years, so lets add it to confusion_df
confusion_df["marketValueYear"] = data["marketValueYear"]
confusion_df.head()

data.drop(columns="marketValueYear", inplace=True)
data.head()

# Now since the marketTotalValue is the addition of marketLandValue and marketImprovementValue, we can just keep the marketTotalValue,
# which is the label to be predicted by our regresison model

housePrice = data.marketTotalValue.rename("housePrice")
print(housePrice)

# We keep the marketLandValue and marketImprovementValue into the confusion_df, and remove from data
confusion_df["marketLandValue"] = data["marketLandValue"]
confusion_df["marketImprovementValue"] = data["marketImprovementValue"]
confusion_df.head()

data = data.drop(columns=["marketLandValue", "marketImprovementValue", "marketTotalValue"])
data.head()

print(data.landUseGeneral.unique())
print(data.landUseCode.unique())

# What is the meaning of landUseGeneral and landUseCode? Lets add them to the "confusion_df" for now

confusion_df = pd.concat([confusion_df, data.iloc[:,0:2]], axis=1)
confusion_df.head()

data.drop(columns=["landUseGeneral", "landUseCode"], inplace=True)
data.head()

print(data["landUseDescription"].unique())

# We keep the landUseDescription in the new_data
new_data["landUseDescription"] = data.landUseDescription
new_data.head()

data.drop(columns=["landUseDescription"], inplace=True)
data.head()

print(data["zoningDescription"].unique())

# We keep the zoningDescription in the new_data
new_data["zoningDescription"] = data.zoningDescription
new_data.head()

data.drop(columns=["zoningDescription"], inplace=True)
data.head()

# We keep the lotSizeAcres, lotSizeSquareFeet	and lotTopography in the new_data
new_data = pd.concat([new_data, data.iloc[:, :3]], axis=1)
new_data.head()

data = data.drop(columns=["lotSizeAcres", "lotSizeSquareFeet", "lotTopography"])
data.head()

print(data.numberOfBuildings.unique())

# Which number is this talking about? Buildings in a neighbourhood or building owned by the apartment? Should we separate houses and buildings?
# For now, lets keep it in confusion_df
confusion_df["numberOfBuildings"] = data.numberOfBuildings
confusion_df.head()

data.drop(columns="numberOfBuildings", inplace=True)
data.head()

"""## Dump Data"""

# We don't need url and id, so we add them to dump_df
dump_df["url"] = data["url"]
dump_df["id"] = data["id"]
dump_df.head()

data.drop(columns=["url", "id"], inplace=True)
data.head()

print(data["building.noOfUnits"].unique())

# No. of units is confusing, so lets keep it in to the confusion_df
confusion_df["noOfUnits"] = data["building.noOfUnits"]
confusion_df.head()

data.drop(columns="building.noOfUnits", inplace=True)
data.head()

"""## Confusion Data"""

# What decides the building quality? A, B, C ???
# Lets add it into the confusion_df
confusion_df["quality"] = data["building.quality"]
confusion_df.head()

data.drop(columns="building.quality", inplace=True)
data.head()

# I think we can incoroporate:
# building condition, architecturalStyle, yearBuild, totalStories, totalRooms, totalbedrooms, baths, heating, ac, foundation, fireplace, water, sewer
# so lets add them to new_data
temp_df = data.iloc[:,:]
temp_cols = list(temp_df.columns)
print(temp_cols)
new_cols = list(map(lambda x: x.split('.')[1], temp_cols))
print(new_cols)
temp_cols_dict = dict(zip(temp_cols, new_cols))
print(temp_cols_dict)

"""## New Data"""

temp_df = temp_df.rename(columns=temp_cols_dict)
new_data = pd.concat([new_data, temp_df], axis=1)
new_data.head()

data.drop(columns=temp_cols, inplace=True)
data.head()

# Nw we concat the housePrice label with the important features "new_data"
filtered_data = pd.concat([new_data, housePrice], axis=1)
filtered_data.head()

import os
if not os.path.exists("/content/Data"):
  os.mkdir("/content/Data")

filtered_data.to_csv("Data/filtered_data.csv", index=False)
confusion_df.to_csv("Data/confusion_data.csv", index=False)
dump_df.to_csv("Data/dump_data.csv", index=False)

"""## Notes

We processed all the individual features, and finally we have 4 dataframes and 1 series:

Dataframes
1. new_data: the collection of important features
2. confusion_df: the confusing features that need to be discussed
3. dump_df: the irrelevant features
4. data: empty data, filtered from the raw data

Series
1. housePrice: series of housing price that will be used as label to be predicted

# Step 2: Correlation Analysis and Cleaning

## Loading Packages and Data
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

!gdown --id 1BtEJUQdpBHTLfk6qPiXuW7eAei5XeEze

data = pd.read_csv("filtered_data.csv")

data.head()

data = data.drop(columns=["zip4"], axis=1)

data["zip4"].value_counts(dropna=False)

(data.dropna(subset=["street"])["city"])

data["zip"].value_counts(dropna=False)

(len(data["street"]))

data["street"].value_counts(dropna=False)

list(data["streetSuffix"].str.lower().value_counts(dropna=False)).count("nan")

[' '.join(i) for i in zip(data["street"].map(str),data["streetSuffix"].map(str))]

pd.concat([data["street"], data["streetSuffix"]])

data["city"].unique()

!gdown --id 1OxS3PzFvj5QIhFdBdscv6JBupg3CjzBN

states = pd.read_csv("States.csv")["States"]

states.head()

from sklearn.preprocessing import LabelEncoder

data["city"] = data["city"].str.lower()

data["city"].head()

data["city"].value_counts(dropna=False)

data = data.dropna(subset=["city"])

data.head()

def label_encode(data, column):
  le = LabelEncoder()
  le.fit(data[column])
  data[column] = le.fit_transform(data[column])
  return data

data["city"].unique()

len(data["street"].str.lower().unique())

data["street"].value_counts(dropna=False)

data = label_encode(data, "street")

# Dropping "zip4", "unit", "foundation", "sewer"  column
data.drop(columns=["zip4", "unit", "foundation", "sewer"], inplace=True)

data.head()

# Now we remove all the NaNs belonging to categorical features, since we have huge amount of data
data = data.dropna(subset=["city", "state", "street", "streetSuffix", "landUseDescription", "zoningDescription", "lotTopography","condition","architecturalStyle", "heating", "fireplace"])

data.isna().sum()

data.describe()

"""## Correlation Analysis"""

corr = data.corr()
corr

def check_correlation(feature_df, target_df):
  r = feature_df.corr(target_df)
  print(f"The correlation coefficient is: {r}.")

feature = "longitude"
target = "lotSizeSquareFeet"
check_correlation(data[feature], data[target])

data["heating"].value_counts()

"""## Notes

Here, we found out the correlation of different features with the housing price. We also analysed the interdependance of the features with one another. And removed the irrelevant features.

# Step 3: Data Encoding and Normalization

## Loading packages and data
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from pickle import dump

!gdown --id 1BtEJUQdpBHTLfk6qPiXuW7eAei5XeEze

data = pd.read_csv("filtered_data.csv")

data.head()

# Dropping "state", "zip4", "unit", "foundation", "sewer", "water", "fireplace", "heating", "airConditioning", "architecturalStyle"
data.drop(columns=["state","zip4", "unit", "foundation", "sewer", "water", "fireplace", "heating", "airConditioning", "architecturalStyle"], inplace=True)

data.head()

# Counting the number of NaN in the dataframe
data.isna().sum()

# Now we remove all the NaNs belonging to categorical features, since we have huge amount of data
cols = list(data.columns)
data = data.dropna(subset=cols)

# Checking if there is any NaN or not
data.isna().sum()

data.info()

data.describe()

data.head()

"""## Encoding

### Label Encoding
"""

def label_encode(data, column):
  data_copy = data.copy()
  data_copy[column] = data_copy[column].str.lower()
  le = LabelEncoder()
  le.fit(data_copy[column])
  data_copy[column] = le.fit_transform(data_copy[column])
  
  noClasses = le.classes_
  encodings = list(range(len(noClasses)))
  encoding_map = dict(zip(noClasses, encodings))

  return data_copy, encoding_map

"""### Encoding Database"""

categorical_cols = ["city", "street", "streetSuffix", "landUseDescription", "zoningDescription", "lotTopography", "condition"]
numerical_cols =	["house", "zip", "latitude", "longitude", "lotSizeAcres",	"lotSizeSquareFeet",	"yearBuilt",	"totalStories",	"totalRooms",	"bedrooms",	"baths"]

encodings_database = dict()
for column in categorical_cols:
  data, encoding_map = label_encode(data, column)
  encodings_database[column] = encoding_map

encodings_database.keys()

encodings_database.values()

for key, values in encodings_database.items():
  print(f"{key}: {len(values)}")

assert len(encodings_database.keys()) == len(encodings_database.values())

data.head()

"""## Normalization

### MinMax Scaling
"""

# Now we will be using MinMax Scalar for the Categorical Features
categorical_data = data[categorical_cols]

minMaxScalar = MinMaxScaler()
data[categorical_cols] = minMaxScalar.fit_transform(categorical_data)

data.head()

"""### Standard Scaling (Z-Score)"""

# We use Standard Scalar to scale our numerical data since we have the continuous values without any maximum and minimum values.
numerical_data = data[numerical_cols]

standardScalar = StandardScaler()
data[numerical_cols] = standardScalar.fit_transform(numerical_data)

data.head()

data.to_csv("Processed Data.csv", index=False)

dump(minMaxScalar, open("minMaxScalar.pkl", "wb"))
dump(standardScalar, open("standardScalar.pkl", "wb"))

"""## Notes

**Removing Columns:**

* Removed columns **unit**, **foundation**, **sewer**, **architecturalStyle** because of huge amount of NaN values:
  * *unit*: 260174
  * *foundation*: 239294
  * *sewer*: 115998
  * *architecturalStyle*: 83644
* Removed **zip4**: Since it was found less correlation with the housing price: -0.009316
* Removed **state**: Since we have the data of only the state GA, out model won't get affected with the state. We can add it only if we have the data from multiple states.
* Removed **water**: Since it has only given "Yes" and "NaN" which is unclear.
* Removed **Fireplace**: Since it has only single value "Masonry"
* Removed **Heating**: Since 99% data is labelled as "Forced Air"
* Removed **airConditioning**: Since 99% data is labelled as "Yes"

**Dropped NaN for the following features features**:

The features are:
  * **Categorical Features**:
    1.  *City*
    2.  *Street Name*
    3.  *Street Suffix*
    4.  *Land Use Description*
    5.  *Zoning Description*
    6.  *Lot Topography*
    7.  *Condition*

  * **Numerical Features**
    1.  *Zip*
    2.  *Latitude*
    3.  *Longitude*	
    4.  *LotSizeAcres*
    5.  *LotSizeSquareFeet*
    6.  *Year Built*
    7.  *Total Stories*
    8.  *Total Rooms*
    9.  *Bedrooms*
    10. *Baths*
    11. *House Number*
  
  * **Target**
    1. *housePrice*

**For our regression model, we have followings:**
  * *Categorical Features*: 7
  * *Numerical Features*: 11
  * *Target*: 1
  * *Total Data*: 217026


Now we encode the categorical features into numbers as follows:

**Encoding Categorical Features:**

* Converted the data into lower case
* Used **Label Encoding** to assign the number for the street as provided in our dataset (starting from 0)
* Thus, as per our data, the categorical features are in the following inclusive ranges:
  * *City*: [0 - 19]
  * *Street*: [0 - 8152]
  * *Street Suffix*: [0 - 62]
  * *Land Use Description*: [0 - 15]
  * *Zoning escription*: [0 - 84]
  * *Lot Topography*: [0 - 4]
  * *Condition*: [0 - 4]

**Normalization**

* For the *categorical features*, we have the minimum and maximum values since we have indexed from (0 - maximum). Therefore, we use the **MinMaxScaling** to scale the categorical data between 0 - 1.

* For the *numerical features*, the values are continuous and we do not have the maximum and minimum values. Therefore, we use the Standard Scaling, which computes the mean and variance based on the standard scaling technique.

* Finally, we dump the scaling models as the pickle files which can be used to scale the values of the features on predictions.
"""



"""# Step 4: Data Preprocessing - After Class Update

## Loading Packages and Data
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
import os
import json
# from pickle import dump
from google.colab import files

!gdown --id 11n0QOY5LuFQBfZSYrwJuQWVtYohFMjoy

data = pd.read_csv("filtered_data.csv")

data.head()

"""## Replacing NaN by "Others""""

data.architecturalStyle.value_counts(dropna=False)

data["architecturalStyle"] = data["architecturalStyle"].fillna("Others")

data.architecturalStyle.value_counts(dropna=False)

data.heating.value_counts(dropna=False)

# Because we don't know what type of heating is "Yes"
data = data.drop(data[data["heating"] == "Yes"].index)

data["heating"] = data["heating"].fillna("Others")

data.heating.value_counts(dropna=False)

data.fireplace.value_counts(dropna=False)

data["fireplace"] = data["fireplace"].fillna("Others")

data.fireplace.value_counts(dropna=False)

data.airConditioning.value_counts(dropna=False)

data["airConditioning"] = data["airConditioning"]

data = data.drop(data[data["airConditioning"] == "Partial"].index)

data.airConditioning.value_counts(dropna=False)

data.foundation.unique()

data["foundation"] = data["foundation"].fillna("Others")

data.foundation.unique()

housePrice = data["housePrice"]
data.drop(columns="housePrice", inplace=True)

data["soldYear"] = 2021

data.head()

data["housePrice"] = housePrice

data.head()

data.drop(columns=["unit", "zip4", "water", "sewer", "house"], inplace=True)

data.head()

data["city"].value_counts(dropna=False)

len(data["city"].unique())

data["city"] = data["city"].fillna("Others")

data.city.value_counts(dropna=False)

len(data["city"].unique())

data["street"].value_counts(dropna=False)

data["street"] = data["street"].fillna("Others")

data["street"].value_counts(dropna=False)

len(data["street"].unique())

data["streetSuffix"].value_counts(dropna=False)

data["streetSuffix"].isna().sum()

data["streetSuffix"] = data["streetSuffix"].fillna("Others")

data.isna().sum()

data["zoningDescription"] = data["zoningDescription"].fillna("Others")

data["landUseDescription"] = data["landUseDescription"].fillna("Others")

data["lotTopography"] = data["lotTopography"].fillna("Others")

data.isna().sum()

cols = data.columns
data.dropna(subset=cols, inplace=True)

data.describe()

data.shape

def label_encode(data, column):
  data_copy = data.copy()
  data_copy[column] = data_copy[column].str.lower()
  le = LabelEncoder()
  le.fit(data_copy[column])
  data_copy[column] = le.fit_transform(data_copy[column])
  
  noClasses = le.classes_
  encodings = list(range(len(noClasses)))
  encoding_map = dict(zip(noClasses, encodings))

  return data_copy, encoding_map

categorical_cols = ["city", "street", "streetSuffix", "landUseDescription", "zoningDescription", "lotTopography", "architecturalStyle", "condition", "heating", "airConditioning",	"foundation", "fireplace"]
numerical_cols = ["zip", "latitude", "longitude", "lotSizeAcres",	"lotSizeSquareFeet",	"yearBuilt",	"totalStories",	"totalRooms",	"bedrooms",	"baths", "soldYear"]

data.head()

encodings_database = dict()
for column in categorical_cols:
  data, encoding_map = label_encode(data, column)
  encodings_database[column] = encoding_map

"""## Encoding State"""

# Encoding State
states = ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN",
          "IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV",
          "NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN",
          "TX","UT","VT","VA","WA","WV","WI","WY"]

states = list(map(str.lower, states))
states_dict = dict(zip(states, list(range(50))))
print(states_dict)
encodings_database["state"] = states_dict

for state in data.state.unique():
  data["state"][data["state"] == state] = encodings_database["state"][state.lower()]

data["state"]

data.head()

encodings_database.keys()

encodings_database.values()

for key, values in encodings_database.items():
  print(f"{key}: {len(values)}")

assert len(encodings_database.keys()) == len(encodings_database.values())

data.head()

data.corr()

data["state"]

# NORMALIZATION:
# Now we will be using MinMax Scalar for the Categorical Features
categorical_data = data[categorical_cols]

minMaxScalar = MinMaxScaler()
data[categorical_cols] = minMaxScalar.fit_transform(categorical_data)
data["state"] = data["state"]/49  # For state: Min: 0, Max: 49

data.head()

# Converting the dataframe to numeric
data = data.apply(pd.to_numeric, errors="coerce")

# We use Standard Scalar to scale our numerical data since we have the continuous values without any maximum and minimum values.
numerical_data = data[numerical_cols]

standardScalar = StandardScaler()
data[numerical_cols] = standardScalar.fit_transform(numerical_data)

if not os.path.exists("/content/Output"):
  os.mkdir("/content/Output")

with open("/content/Output/encodings_database.json", "w") as f:
  json.dump(encodings_database, f, indent=3)

data.to_csv("/content/Output/processedData.csv", index=False)

from joblib import dump

dump(minMaxScalar, "/content/Output/minMaxScalar.joblib")
dump(standardScalar, "/content/Output/standardScalar.joblib")

BASE_DIR = "/content/Output/"
filenames = os.listdir(BASE_DIR)
for filename in filenames:
  file_path = os.path.join(BASE_DIR, filename)
  files.download(file_path)

"""## Notes

Since we have only the data for Gwinnett County of GA, the model may be more specific to that county. But we have tried to generalized to other places as well, by introducing the term "Others" for the state, cities, street other than in the Gwinnett County.

We did the following in this notebook:
* We incorporated "**Architectural Style**", "**Fireplace**", "**Heating**" and "**airConditioning**".
* We added two featues i.e. **soldYear** and **lastPriceSold** which are the simulated data from the actualMarketValue.
* Then we encoded all the categorical features using SKLearn **Label Encoder**, and for the **states** we explicitly encoded the values.
* The label encodings are saved in a **JSON file** which provided the information of the mapping of categorical features with exact index used while data preprocessing.
* After that, we normalized the categorical data using **MinMaxScalar** since we had the minimum and maximum values after the label encoding part. 
* For the numerical data, we performed the **Standard Scaling Normalization**, by obtaining the mean and standard deviation.
* The minMaxScalar and standardScalar are dumped into a **pickle file** to use them again while preprocessing the test data.

#Step 5: Outlier Removal and Feature Engineering

## Loading Packages and Data
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
import os
import json
# from pickle import dump
from google.colab import files

!gdown --id 1BtEJUQdpBHTLfk6qPiXuW7eAei5XeEze

data = pd.read_csv("filtered_data.csv")

data.head()

data.architecturalStyle.value_counts(dropna=False)

data["architecturalStyle"] = data["architecturalStyle"].fillna("Others")

data.architecturalStyle.value_counts(dropna=False)

data.heating.value_counts(dropna=False)

# Because we don't know what type of heating is "Yes"
data = data.drop(data[data["heating"] == "Yes"].index)

data["heating"] = data["heating"].fillna("Others")

data.heating.value_counts(dropna=False)

data.fireplace.value_counts(dropna=False)

data["fireplace"] = data["fireplace"].fillna("Others")

data.fireplace.value_counts(dropna=False)

data.airConditioning.value_counts(dropna=False)

data["airConditioning"] = data["airConditioning"]

data = data.drop(data[data["airConditioning"] == "Partial"].index)

data.airConditioning.value_counts(dropna=False)

data.foundation.unique()

data["foundation"] = data["foundation"].fillna("Others")

data.foundation.unique()

housePrice = data["housePrice"]
data.drop(columns="housePrice", inplace=True)

data["soldYear"] = 2021

data.head()

data["housePrice"] = housePrice

data.head()

data.drop(columns=["unit", "zip4", "water", "sewer", "house"], inplace=True)

data.head()

data["city"].value_counts(dropna=False)

len(data["city"].unique())

data["city"] = data["city"].fillna("Others")

data.city.value_counts(dropna=False)

len(data["city"].unique())

data["street"].value_counts(dropna=False)

data["street"] = data["street"].fillna("Others")

data["street"].value_counts(dropna=False)

len(data["street"].unique())

data["streetSuffix"].value_counts(dropna=False)

data["streetSuffix"].isna().sum()

data["streetSuffix"] = data["streetSuffix"].fillna("Others")

data.isna().sum()

data["zoningDescription"] = data["zoningDescription"].fillna("Others")

data["landUseDescription"] = data["landUseDescription"].fillna("Others")

data["lotTopography"] = data["lotTopography"].fillna("Others")

data.isna().sum()

cols = data.columns
data.dropna(subset=cols, inplace=True)

data.describe()

data.shape

"""## Outlier Removal

### House Price
"""

import numpy as np
iqr_hp = np.quantile(data['housePrice'],0.75) - np.quantile(data['housePrice'],0.25)
lower_out_hp = np.quantile(data['housePrice'],0.25) - 1.5*iqr_hp
higher_out_hp = np.quantile(data['housePrice'],0.75) + 1.5*iqr_hp
data = data.drop(data[data["housePrice"] > higher_out_hp].index) 
data = data.drop(data[data["housePrice"] < lower_out_hp].index)
data.head()

data.describe()

data.shape

"""### Lot Size Square Feet"""

iqr_lsf = np.quantile(data['lotSizeSquareFeet'],0.75) - np.quantile(data['lotSizeSquareFeet'],0.25)
lower_out_lsf = np.quantile(data['lotSizeSquareFeet'],0.25) - 1.5*iqr_lsf
higher_out_lsf = np.quantile(data['lotSizeSquareFeet'],0.75) + 1.5*iqr_lsf
data = data.drop(data[data["lotSizeSquareFeet"] > higher_out_lsf].index) 
data = data.drop(data[data["lotSizeSquareFeet"] < lower_out_lsf].index)
data.describe()



data.head()

"""### Total Rooms"""

iqr_tr = np.quantile(data['totalRooms'],0.75) - np.quantile(data['totalRooms'],0.25)
#lower_out_tr = np.quantile(data['totalRooms'],0.25) - 1.5*iqr_tr
higher_out_tr = np.quantile(data['totalRooms'],0.75) + 1.5*iqr_tr
data = data.drop(data[data["totalRooms"] > higher_out_tr].index) 
#data = data.drop(data[data["lotSizeSquareFeet"] < lower_out_lsf].index)
data.describe()

data.head()

data.to_csv("to_bimal_sir.csv", index=False)

def label_encode(data, column):
  data_copy = data.copy()
  data_copy[column] = data_copy[column].str.lower()
  le = LabelEncoder()
  le.fit(data_copy[column])
  data_copy[column] = le.fit_transform(data_copy[column])
  
  noClasses = le.classes_
  encodings = list(range(len(noClasses)))
  encoding_map = dict(zip(noClasses, encodings))

  return data_copy, encoding_map

categorical_cols = ["city", "street", "streetSuffix", "landUseDescription", "zoningDescription", "lotTopography", "architecturalStyle", "condition", "heating", "airConditioning",	"foundation", "fireplace"]
numerical_cols = ["zip", "latitude", "longitude", "lotSizeAcres",	"lotSizeSquareFeet",	"yearBuilt",	"totalStories",	"totalRooms",	"bedrooms",	"baths", "soldYear"]

data.head()

encodings_database = dict()
for column in categorical_cols:
  data, encoding_map = label_encode(data, column)
  encodings_database[column] = encoding_map

# Encoding State
states = ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN",
          "IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV",
          "NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN",
          "TX","UT","VT","VA","WA","WV","WI","WY"]

states = list(map(str.lower, states))
states_dict = dict(zip(states, list(range(50))))
print(states_dict)
encodings_database["state"] = states_dict

for state in data.state.unique():
  data["state"][data["state"] == state] = encodings_database["state"][state.lower()]

data["state"]

data.head()

encodings_database.keys()

encodings_database.values()

for key, values in encodings_database.items():
  print(f"{key}: {len(values)}")

assert len(encodings_database.keys()) == len(encodings_database.values())

data.head()

data.corr()

data["state"]

# NORMALIZATION:
# Now we will be using MinMax Scalar for the Categorical Features
categorical_data = data[categorical_cols]

minMaxScalar = MinMaxScaler()
data[categorical_cols] = minMaxScalar.fit_transform(categorical_data)
data["state"] = data["state"]/49  # For state: Min: 0, Max: 49

data.head()

# Converting the dataframe to numeric
data = data.apply(pd.to_numeric, errors="coerce")

# We use Standard Scalar to scale our numerical data since we have the continuous values without any maximum and minimum values.
numerical_data = data[numerical_cols]

standardScalar = StandardScaler()
data[numerical_cols] = standardScalar.fit_transform(numerical_data)

if not os.path.exists("/content/Output"):
  os.mkdir("/content/Output")

with open("/content/Output/encodings_database.json", "w") as f:
  json.dump(encodings_database, f, indent=3)

data.to_csv("/content/Output/processedData.csv", index=False)

from joblib import dump

dump(minMaxScalar, "/content/Output/minMaxScalar.joblib")
dump(standardScalar, "/content/Output/standardScalar.joblib")

BASE_DIR = "/content/Output/"
filenames = os.listdir(BASE_DIR)
for filename in filenames:
  file_path = os.path.join(BASE_DIR, filename)
  files.download(file_path)

"""## Notes

Outliers are the extreme values that are not representative of the majority of data and normally outside the range of expected values. In machine learning, it's important to understand the outliers and in most cases need to remove outliers to train the model with representative data.

There are various approaches to remove outliers such as Z-score, Interquartile Range, DBScan, Isolation Forests etc. In this project we first tried popular approach that is widely applauded by ML communities i.e. Z-Score, also called outlier removal using standard deviation.

In the Z-score approach, standard deviation can be used as a cut-off point to remove outliers in normal or gaussian like distribution. One standard deviation includes 68% of data, two standard deviations includes 95% of data and three standard deviations includes 99.7% data points. i.e. if μ is mean and σ is standard deviation then,

* μ ± σ ⇒ 68%
* μ ± 2σ ⇒ 95%
* μ ± 3σ ⇒ 99.7%

In this project, first, we calculated mean and standard deviation for the selected numerical features such as House Price, LotSizeSquareFeet, Total Rooms where we manually observed outliers .Once feature’s mean and standard deviation computed, we removed outliers with 3 standard deviations, 2.5 standard deviations and 2 standard deviations and trained the model separately. We achieved some improvements in our training and test accuracies by removing outliers, but the improvement is not satisfactory in this method. Then we tried another well accepted outlier removal method, Interquartile Range.

In the Interquartile Range, for each selected numerical feature, we calculated the interquartile range IQR = Q3-Q1. Then we removed data points that is lower than Q1 - 1.5 * IQR and higher than Q3 + 1.5 * IQR.

# Step 6: Multiple Regression Models

## Loading Packages and Data
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Support Vector Machine
from sklearn.svm import SVR

# KNN
from sklearn.neighbors import KNeighborsRegressor

# Decision Tree
from sklearn.tree import DecisionTreeRegressor

# ANN
from keras.models import Sequential
from keras.layers import Dense

# RandomForest 
from sklearn.ensemble import RandomForestRegressor

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/outlierRemovedProcessedData.csv")

data.head()

"""## Dataset Split"""

X = data.iloc[:, :-1]
y = data["housePrice"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=27)
X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3, random_state=27)

"""## Defining Regression Models"""

def ANN():
  model = Sequential()
  model.add(Dense(input_dim=24, units=64, activation=None))
  model.add(Dense(units=32, activation=None))
  model.add(Dense(units=16, activation=None))
  model.add(Dense(units=1, activation=None))
  model.compile(loss="mse", metrics=['mae'], optimizer='adam')
  return model

models = {
          "KNN": KNeighborsRegressor(n_neighbors=5),
          "ANN": ANN(),
          "Decision Trees": DecisionTreeRegressor(max_depth=15),
          "Random Forest": RandomForestRegressor(max_depth=23)
          }

def save_and_display(y_true, y_pred, name, prompt):
  plt.scatter(y_true, y_pred, c='w', edgecolors='b')
  plt.plot(y_true, y_true, c='r')
  plt.title(f"Calibration Plot for {name} on {prompt} Data")
  plt.xlabel("Actual House Price")
  plt.ylabel("Predicted House Price")
  plt.savefig(f"Output/cp_{name}_{prompt}.png", dpi=300)
  plt.show()

import os
if not os.path.exists("Output"):
  os.mkdir("Output")

"""## Training and Visualization"""

r2_scores = dict()

for name, model in models.items():
  print(f"Working on {name} Regression") 
  if name == "ANN":
    y_train_ann = y_train/100000
    model.fit(X_train, y_train_ann, epochs=15, verbose=1)
  else:
    model.fit(X_train, y_train)

  train_pred = model.predict(X_train)
  y_pred = model.predict(X_test)

  if name=="ANN":
    train_pred = train_pred * 100000
    y_pred = y_pred * 100000

  train_r2 = round(r2_score(y_train, train_pred), 3)
  print(f"Train R2 using {name}: {train_r2}")

  test_r2 = round(r2_score(y_test, y_pred), 3)
  print(f"Test R2 using {name}: {test_r2}")
  save_and_display(y_train, train_pred, name, "Training")
  save_and_display(y_test, y_pred, name, "Test")
  r2_scores[name] = [train_r2, test_r2]

r2_scores

"""## Comparison of Regression Models"""

def comparision(x_plot, y_plot, prompt):
  plt.figure(figsize=(18,8))
  plt.title(f"R2 Scores on {prompt} Data")
  plt.xlabel("Algorithm")
  plt.ylabel("R2 Score")
  g=sns.barplot(x_plot, y_plot)
  for i,p in enumerate(g.patches):
          percentage = '{:.3f}'.format(y_plot[i])
          x1 = p.get_x() + p.get_width() - 0.5
          y1 = p.get_y() + p.get_height() + 0.02
          g.annotate(percentage, (x1, y1))
  plt.savefig(f"Output/comparison_{prompt}.png", dpi=300)
  plt.show()

x_plot = list(r2_scores.keys())
y_plot_train = [train for train, test in r2_scores.values()]
y_plot_test = [test for train, test in r2_scores.values()]
comparision(x_plot, y_plot_train, "Train")

comparision(x_plot, y_plot_test, "Test")

from joblib import dump

dump(model, "final_rf_model.joblib")

"""# Step 7: Training and Saving Model

## Loading Packages and Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#data = pd.read_csv("/content/drive/MyDrive/[LATEST] Manish New Data/processedData.csv")
data = pd.read_csv("/content/drive/MyDrive/processedData.csv")

data.head()

data.describe()

data.shape

X = data.iloc[:, :-1]
y = data["housePrice"]

print(len(X))
print(len(y))

"""## Train, Test and Validation Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=27)

X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3, random_state=27)

print(len(X_train))
print(len(X_test))

"""## Random Forest Regression Model

### Training
"""

# rf_model = RandomForestRegressor(max_depth=45)
rf_model = RandomForestRegressor(max_depth=23)

rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

train_pred = rf_model.predict(X_train)

"""### R2 - Evaluation"""

train_r2 = round(r2_score(y_train, train_pred), 3)
print(f"Train R2: {train_r2}")

test_r2 = round(r2_score(y_test, y_pred), 3)
print(f"Test R2: {test_r2}")

"""### Visualization"""

plt.scatter(y_train, train_pred, c='w', edgecolors='b')
plt.plot(y_train, y_train, c='r')
plt.xlabel(f"Actual House Price")
plt.ylabel(f"Predicted House Price")
plt.savefig("Training Regression Calibration Plot.png", dpi=300)
plt.show()

plt.scatter(y_test, y_pred, c='w', edgecolors='b')
plt.plot(y_test, y_test, c='r')
plt.xlabel(f"Actual House Price")
plt.ylabel(f"Predicted House Price")
plt.savefig("Validation Regression Calibration Plot.png", dpi=300)
plt.show()

"""### Saving the model"""

dump(rf_model, "rf_model.joblib")

"""# Step 8: Online Learning

## Loading Packages and Data
"""

# Installing the packages

!pip install tensorflow-io
!pip install kafka-python

# Importing the packages

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import r2_score
import time 
from kafka import KafkaProducer
import tensorflow as tf
import tensorflow_io as tfio
# import pickle as pkl
from joblib import load

"""Download and setup Kafka for real time data stream simulation. 

"""

!curl -sSOL https://downloads.apache.org/kafka/2.8.1/kafka_2.12-2.8.1.tgz
!tar -xzf kafka_2.12-2.8.1.tgz

"""Start Kafka and Zookeeper servers as a daemon processes. Zookeeper is a centralized service for maintaing configuration information, naming, providing distributed synchronization, and providing group services."""

!./kafka_2.12-2.8.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-2.8.1/config/zookeeper.properties
!./kafka_2.12-2.8.1/bin/kafka-server-start.sh -daemon ./kafka_2.12-2.8.1/config/server.properties

"""## Create a topic to store partitions
Create topic for train and test dataset to store events in Kafka. Kafka is a distributed event streaming platform that lets you read, write, store and process partitions. These events or messages are organized and stored in topics. In simple terms, topic is similar to a folder in a filesystem, and the message are the file in that folder.
"""

!./kafka_2.12-2.8.1/bin/kafka-topics.sh --create --topic home-train --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1
!./kafka_2.12-2.8.1/bin/kafka-topics.sh --create --topic home-test --bootstrap-server localhost:9092 --replication-factor 1 --partitions 2

"""## Describe the topic for details
Describe command helps us gather details on topic, it's partitions, replicas, and other important information.

"""

!./kafka_2.12-2.8.1/bin/kafka-topics.sh --describe --topic home-train --bootstrap-server localhost:9092
!./kafka_2.12-2.8.1/bin/kafka-topics.sh --describe --topic home-test --bootstrap-server localhost:9092

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/Software Engg Regression Analysis/[LATEST] Manish New Data/onlineLearningData.csv")

data.head(10)

train_df, test_df = train_test_split(data, test_size=0.3, shuffle=True)
test_df, validate_df = train_test_split(test_df, test_size= 0.3, shuffle= True)
x_train_df = train_df.drop(["housePrice"], axis=1)
y_train_df = train_df["housePrice"]

x_test_df = test_df.drop(["housePrice"], axis=1)
y_test_df = test_df["housePrice"]

x_validate_df = validate_df.drop(["housePrice"], axis = 1)
y_validate_df = validate_df["housePrice"]

print("Number of training samples: ",len(train_df))
print("Number of testing sample: ",len(test_df))

"""## Convert data to list format
Read each row from the dataframe and convert it to the list format to feed to Kafka.
"""

#Convert each test and train dataframe to list form to feed to kafka
x_train = list(filter(None, x_train_df.to_csv(index=False).split("\n")[1:]))
y_train = list(filter(None, y_train_df.to_csv(index=False).split("\n")[1:]))

x_test = list(filter(None, x_test_df.to_csv(index=False).split("\n")[1:]))
y_test = list(filter(None, y_test_df.to_csv(index=False).split("\n")[1:]))
len(x_train), len(y_train), len(x_test), len(y_test)

NUM_COLUMNS = len(x_train_df.columns)

"""## Create Kafka Producer 
Create Kafka producer which takes in data and sends the record to the partition within a topic in Kafka cluster. 
"""

#send each record to a partition within a topic in kafka cluster
def write_to_kafka(topic, items):
  count=0
  producer = KafkaProducer(bootstrap_servers=['localhost:9092'])
  for message, key in items:
    producer.send(topic, key=key.encode('utf-8'), value=message.encode('utf-8'))
    count += 1 
  producer.flush()
  print("Wrote {0} messages into topic: {1}".format(count, topic))

write_to_kafka("home-train", zip(x_train, y_train))
write_to_kafka("home-test", zip(x_test, y_test))

"""## Decode the incoming Online Data
Unlike traditional training of machine learning models, online learning is based on incrementally learning or updating parameters as soon as the new data points are available. This process continues indefinitely. In the code below, stream_timeout is set to 10000 milliseconds which means as all the messages are consumed from the topic, the dataset will wait for 10 more seconds before timing out and disconnecting from the Kafka cluster. If additional data arrives in that time period, model training resumes. 
"""

online_train_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=["home-train"],
    group_id="cgonline",
    servers="localhost:9092",
    stream_timeout=10000, # in milliseconds, to block indefinitely, set it to -1.
    configuration=[
        "session.timeout.ms=7000",
        "max.poll.interval.ms=8000",
        "auto.offset.reset=earliest"
    ],
)

def decode_kafka_online_item(raw_message, raw_key):
  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])
  key = tf.strings.to_number(raw_key)
  return (message, key)

# We decode the data and their corresponding labels, and store them as a simple array

online_train_ds_temp = online_train_ds.map(decode_kafka_online_item)

final_online_data = list()
final_online_label = list()

for data, label in online_train_ds_temp:
  data = np.array(data)
  label = np.array(label)
  final_online_data.append(data)
  final_online_label.append(label)

final_online_data[:5]

final_online_label[:5]

"""## Loading the pretrained Random Forest Model"""

import joblib

rf_model = joblib.load("/content/drive/MyDrive/Software Engg Regression Analysis/[LATEST] Manish New Data/final_rf_model.joblib")

type(rf_model)

"""## Training on New Online Data"""

# Fitting the pretrained random forest model using the online data

rf_model.fit(final_online_data, final_online_label)

"""## Model Evaluation and Visualization"""

# Now testing the new model predictions on the new online data

preds = rf_model.predict(final_online_data)

from sklearn.metrics import r2_score

print(f"R2-Score: {round(r2_score(final_online_label, preds), 3)}")

# Plotting the predictions on new online data

plt.scatter(final_online_label, preds, c='w', edgecolors='b')
plt.plot(final_online_label, final_online_label, c='r')
plt.xlabel(f"Actual House Price")
plt.ylabel(f"Predicted House Price")
plt.savefig("Validation Regression Calibration Plot.png", dpi=300)
plt.show()

"""# Step 9: Testing New User Data [DEMO]

## Loading Packages and Data
"""

import pandas as pd
import json
from joblib import load

import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

user_data_2 = pd.read_csv("/content/drive/MyDrive/Software Engg Regression Analysis/[LATEST] Manish New Data/demo_test_data.csv")

"""## Data Validation [Test Cases]"""

# User 108 and 113
user_id = 108

user_data = user_data_2.loc[user_data_2["id"] == user_id]
user_data

zip_average_price = {30047: 224924.8452,
                     30087: 190259.9042}

zip = int(user_data["zip"])
print(f"User {user_id} belongs to the zip {zip}, and the average price for that zip is {zip_average_price[zip]}")

user_id = int(user_data["id"])
user_data = user_data.iloc[:1, 1:]

user_id

user_data

lotSizqSqFt = user_data["lotSizeSquareFeet"].values

# Loading the Encoding Database to encode the categorical data into numbers

file_path = "/content/drive/MyDrive/Software Engg Regression Analysis/[LATEST] Manish New Data/encodings_database.json"
file = open(file_path, "r")
encodings_database = json.load(file)

categorical_cols = ["city", "state", "street", "streetSuffix", "landUseDescription", "zoningDescription", "lotTopography", "architecturalStyle", "condition", "heating", "airConditioning",	"foundation", "fireplace"]
numerical_cols = ["zip", "latitude", "longitude", "lotSizeAcres",	"lotSizeSquareFeet",	"yearBuilt",	"totalStories",	"totalRooms",	"bedrooms",	"baths", "soldYear"]

"""### Validating Encoding State"""

def encode_state(state):
    '''
    This function returns the label of the code of the entered state.
    Args:
        state: str
        - accepts states in USA

    Returns:
        state_code: int
            - -1 for invalid
            - 0 to 50 for actual state codes
    '''
    state = state.lower()
    if state not in encodings_database["state"].keys():
        print("Only accepts the states of USA!")
        state_code = -1
    else:
        print("State Encoded")
        state_code = encodings_database["state"][state]
    return state_code

# Check whether the state is valid or not
state = list(user_data["state"])[0]
state_code = encode_state(state)
print(state)
print(state_code)

state_code = encode_state("BC")
print(state_code)

"""### Validating Numerical Data"""

def validate_numerical_data(num_col_val):
    '''
    This function validates whether the input numerical column actually has the numerical data
    Args:
        num_col_val: str
            - string with an integer e.g. '56'
    Returns:
        number_flag: bool
            - True if all the data are real numbers
            - False if the data is other than real number
    '''
    valid_number_flag = True
    try:
        float(num_col_val)
    except:
        valid_number_flag = False
        print("Only accepts Numerical Data!")
    else:
        print("Numerical Data: Detected and Validated")

    return valid_number_flag

for j, col in enumerate(numerical_cols):
    valid_number_flag = validate_numerical_data(list(user_data[col])[0])
    print(list(user_data[col])[0])
    print(valid_number_flag)
    print("\n")

valid_number_flag = validate_numerical_data("Masonry")
print(valid_number_flag)

"""### Validating Categorical Data"""

def validate_categorical_data(cat_col_val):
    '''
    This function validates whether the input categorical column actually has the categorical data
    Args:
        cat_col_val: str
            - actual string value
    Returns:
        valid_categorical_flag: bool
            - True if all the data are strings
            - False if the data is other than strings
    '''
    valid_categorical_flag = True
    try:
        float(cat_col_val)
    except:
        if bool(cat_col_val) is False:
            valid_categorical_flag = False
            return valid_categorical_flag
        print("Categorical Data: Detected and Validated!")
    else:
        print("Only accepts Categorical Features!")
        valid_categorical_flag = False

    return valid_categorical_flag

for j, col in enumerate(categorical_cols):
    valid_categorical_flag = validate_categorical_data(list(user_data[col])[0])
    print(list(user_data[col])[0])
    print(valid_categorical_flag)
    print("\n")

valid_categorical_flag = validate_categorical_data(5)
print(valid_categorical_flag)

user_data

"""## Data Preprocessing

### Encoding the Categorical Features
"""

for i, name in enumerate(categorical_cols):
  user_data[name] = encodings_database[name][user_data[name].values[0].lower()]

user_data

"""### MinMax Scaling"""

# Loading the MinMaxScalar to normalize the categorical features

filename = "/content/drive/MyDrive/Software Engg Regression Analysis/[LATEST] Manish New Data/minMaxScalar.joblib"
minMaxScalar = load(filename)

# MINMAX NORMALIZATION:

categorical_cols.remove("state")
categorical_data = user_data[categorical_cols]

user_data[categorical_cols] = minMaxScalar.transform(categorical_data)

#Encoding state separately, since we did not have all the 50 states in the data
user_data["state"] /= 49

user_data

"""### Standard Scaling"""

# Now we have all numerical values but in string format, so we convert all the values to numeric
user_data = user_data.apply(pd.to_numeric, errors="coerce")

# Loading the StandardScalar to normalize the numerical features

filename = "/content/drive/MyDrive/Software Engg Regression Analysis/[LATEST] Manish New Data/standardScalar.joblib"
standardScalar = load(filename)

numerical_data = user_data[numerical_cols]

user_data[numerical_cols] = standardScalar.transform(numerical_data)

user_data.shape

user_data

"""### Predictions using pretrained Regression Model"""

# Now the new test data is ready to feed into the Random Forest Regression Model
# rf_model = load("/content/drive/MyDrive/Software Engg Regression Analysis/[LATEST] Manish New Data/final_rf_model.joblib"
rf_model = load("/content/drive/MyDrive/Software Engg Regression Analysis/[LATEST] Manish New Data/rf_model_95_86_randomState27.joblib")

pred = rf_model.predict(user_data.values)
pred = round(pred[0], 3)
print("Predicted Housing Price:", pred)

# Dividing the predicted housing price by lotSizeSquareFeet

predPerSqFt = round((pred/lotSizqSqFt)[0], 3)
print(f"House Price per SqFt: {predPerSqFt}")

"""### Amortization Table and Equity Value"""

amortization_table = pd.read_csv("/content/drive/MyDrive/Software Engg Regression Analysis/[LATEST] Manish New Data/demo_amortization_table.csv")

amortization_table

user_amortization_data = amortization_table.loc[amortization_table["id"] == user_id]
user_amortization_data

user_amortization_data["Predicted House Price"] = pred

user_amortization_data.head()

user_amortization_data["Predicted Square Feet Price"] = predPerSqFt

user_amortization_data["Equity Value"] = user_amortization_data["Predicted House Price"] - user_amortization_data["Remaining Principal"]

user_amortization_data

"""### Encrypting the Predicted Values"""

!pip install cryptography

# Encrypting the data to get the cipher text using Fernet Encryption

from cryptography.fernet import Fernet

# Message to encrypt
id = str(int(user_amortization_data["id"]))
house_price = str(float(user_amortization_data["Predicted House Price"]))
house_price_per_sqft = str(float(user_amortization_data["Predicted Square Feet Price"]))
equity_value = str(round(float(user_amortization_data["Equity Value"]), 3))

# Generating the Key
key = Fernet.generate_key()
fernet = Fernet(key)

print(f"Encryption Key: {key}\n")

# Encrpyting the messages
enc_id = fernet.encrypt(id.encode())
enc_house_price = fernet.encrypt(house_price.encode())
enc_equity_value = fernet.encrypt(equity_value.encode())
enc_house_price_per_sqft = fernet.encrypt(house_price_per_sqft.encode())

print("User ID: ", id)
print(f"Encrypted User ID: {enc_id}\n")

print("Predicted House Price: ", house_price)
print(f"Encrypted House Price: {enc_house_price}\n")

print("Predicted House Price Per Sqft: ", house_price_per_sqft)
print(f"Encrypted House Price Per Sqft:  {enc_house_price_per_sqft}\n")

print("Predicted Equity Value: ", equity_value)
print(f"Encrypted Equity Value: {enc_equity_value}\n")

print(f"User {user_id} belongs to the zip {zip}, and the average price for that zip is {zip_average_price[zip]}")

"""### Decrypting the Cipher Text"""

dec_id = fernet.decrypt(enc_id).decode()
dec_house_price = fernet.decrypt(enc_house_price).decode()
dec_equity_value = fernet.decrypt(enc_equity_value).decode()
dec_house_price_per_sqft = fernet.decrypt(enc_house_price_per_sqft).decode()

print(f"Decrypted User ID: {dec_id}")
print("Decrypted House Price: ", dec_house_price)
print("Decrypted House Price Per Sqft: ", dec_house_price_per_sqft)
print("Decrypted Equity Value: ", dec_equity_value)