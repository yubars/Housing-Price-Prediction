# -*- coding: utf-8 -*-
"""Step 3 More Data Cleaning and Wrangling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bjRCy8ywBCIUb1hsthZRMNztKqj5JsNh

# Data Preprocessing Before Class
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from pickle import dump

# !gdown --id 1BtEJUQdpBHTLfk6qPiXuW7eAei5XeEze

data = pd.read_csv("filtered_data.csv")

data.head()

# Dropping "state", "zip4", "unit", "foundation", "sewer", "water", "fireplace", "heating", "airConditioning", "architecturalStyle"
data.drop(columns=["state","zip4", "unit", "foundation", "sewer", "water", "fireplace", "heating", "airConditioning", "architecturalStyle"], inplace=True)

data.head()

# Counting the number of NaN in the dataframe
data.isna().sum()

# Now we remove all the NaNs belonging to categorical features, since we have huge amount of data
cols = list(data.columns)
data = data.dropna(subset=cols)

# Checking if there is any NaN or not
data.isna().sum()

data.info()

data.describe()

data.head()

def label_encode(data, column):
  data_copy = data.copy()
  data_copy[column] = data_copy[column].str.lower()
  le = LabelEncoder()
  le.fit(data_copy[column])
  data_copy[column] = le.fit_transform(data_copy[column])
  
  noClasses = le.classes_
  encodings = list(range(len(noClasses)))
  encoding_map = dict(zip(noClasses, encodings))

  return data_copy, encoding_map

categorical_cols = ["city", "street", "streetSuffix", "landUseDescription", "zoningDescription", "lotTopography", "condition"]
numerical_cols =	["house", "zip", "latitude", "longitude", "lotSizeAcres",	"lotSizeSquareFeet",	"yearBuilt",	"totalStories",	"totalRooms",	"bedrooms",	"baths"]

encodings_database = dict()
for column in categorical_cols:
  data, encoding_map = label_encode(data, column)
  encodings_database[column] = encoding_map

encodings_database.keys()

encodings_database.values()

for key, values in encodings_database.items():
  print(f"{key}: {len(values)}")

assert len(encodings_database.keys()) == len(encodings_database.values())

data.head()

# NORMALIZATION:
# Now we will be using MinMax Scalar for the Categorical Features
categorical_data = data[categorical_cols]

minMaxScalar = MinMaxScaler()
data[categorical_cols] = minMaxScalar.fit_transform(categorical_data)

data.head()

# We use Standard Scalar to scale our numerical data since we have the continuous values without any maximum and minimum values.
numerical_data = data[numerical_cols]

standardScalar = StandardScaler()
data[numerical_cols] = standardScalar.fit_transform(numerical_data)

data.head()

data.to_csv("Processed Data.csv", index=False)

dump(minMaxScalar, open("minMaxScalar.pkl", "wb"))
dump(standardScalar, open("standardScalar.pkl", "wb"))

"""#NOTES

**Removing Columns:**

* Removed columns **unit**, **foundation**, **sewer**, **architecturalStyle** because of huge amount of NaN values:
  * *unit*: 260174
  * *foundation*: 239294
  * *sewer*: 115998
  * *architecturalStyle*: 83644
* Removed **zip4**: Since it was found less correlation with the housing price: -0.009316
* Removed **state**: Since we have the data of only the state GA, out model won't get affected with the state. We can add it only if we have the data from multiple states.
* Removed **water**: Since it has only given "Yes" and "NaN" which is unclear.
* Removed **Fireplace**: Since it has only single value "Masonry"
* Removed **Heating**: Since 99% data is labelled as "Forced Air"
* Removed **airConditioning**: Since 99% data is labelled as "Yes"

**Dropped NaN for the following features features**:

The features are:
  * **Categorical Features**:
    1.  *City*
    2.  *Street Name*
    3.  *Street Suffix*
    4.  *Land Use Description*
    5.  *Zoning Description*
    6.  *Lot Topography*
    7.  *Condition*

  * **Numerical Features**
    1.  *Zip*
    2.  *Latitude*
    3.  *Longitude*	
    4.  *LotSizeAcres*
    5.  *LotSizeSquareFeet*
    6.  *Year Built*
    7.  *Total Stories*
    8.  *Total Rooms*
    9.  *Bedrooms*
    10. *Baths*
    11. *House Number*
  
  * **Target**
    1. *housePrice*

**For our regression model, we have followings:**
  * *Categorical Features*: 7
  * *Numerical Features*: 11
  * *Target*: 1
  * *Total Data*: 217026


Now we encode the categorical features into numbers as follows:

**Encoding Categorical Features:**

* Converted the data into lower case
* Used **Label Encoding** to assign the number for the street as provided in our dataset (starting from 0)
* Thus, as per our data, the categorical features are in the following inclusive ranges:
  * *City*: [0 - 19]
  * *Street*: [0 - 8152]
  * *Street Suffix*: [0 - 62]
  * *Land Use Description*: [0 - 15]
  * *Zoning escription*: [0 - 84]
  * *Lot Topography*: [0 - 4]
  * *Condition*: [0 - 4]

**Normalization**

* For the *categorical features*, we have the minimum and maximum values since we have indexed from (0 - maximum). Therefore, we use the **MinMaxScaling** to scale the categorical data between 0 - 1.

* For the *numerical features*, the values are continuous and we do not have the maximum and minimum values. Therefore, we use the Standard Scaling, which computes the mean and variance based on the standard scaling technique.

* Finally, we dump the scaling models as the pickle files which can be used to scale the values of the features on predictions.
"""