# -*- coding: utf-8 -*-
"""Step 2 Correlation Analysis and Cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/152YEZwPtJ0AOccVI2SJ83-oDh7o4Y7B7

# Imorting Packages and Loading Data
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

# !gdown --id 1BtEJUQdpBHTLfk6qPiXuW7eAei5XeEze

data = pd.read_csv("filtered_data.csv")

data.head()

"""# Preprocessing Part 1"""

data = data.drop(columns=["zip4"], axis=1)

data["zip4"].value_counts(dropna=False)

(data.dropna(subset=["street"])["city"])

data["zip"].value_counts(dropna=False)

(len(data["street"]))

data["street"].value_counts(dropna=False)

list(data["streetSuffix"].str.lower().value_counts(dropna=False)).count("nan")

[' '.join(i) for i in zip(data["street"].map(str),data["streetSuffix"].map(str))]

pd.concat([data["street"], data["streetSuffix"]])

data["city"].unique()

# !gdown --id 1OxS3PzFvj5QIhFdBdscv6JBupg3CjzBN

states = pd.read_csv("States.csv")["States"]

states.head()

from sklearn.preprocessing import LabelEncoder

data["city"] = data["city"].str.lower()

data["city"].head()

data["city"].value_counts(dropna=False)

data = data.dropna(subset=["city"])

data.head()

def label_encode(data, column):
  le = LabelEncoder()
  le.fit(data[column])
  data[column] = le.fit_transform(data[column])
  return data

data["city"].unique()

len(data["street"].str.lower().unique())

data["street"].value_counts(dropna=False)

data = label_encode(data, "street")

"""# Preprocessing Part 2"""

# Dropping "zip4", "unit", "foundation", "sewer"  column
data.drop(columns=["zip4", "unit", "foundation", "sewer"], inplace=True)

data.head()

# Now we remove all the NaNs belonging to categorical features, since we have huge amount of data
data = data.dropna(subset=["city", "state", "street", "streetSuffix", "landUseDescription", "zoningDescription", "lotTopography","condition","architecturalStyle", "heating", "fireplace"])

data.isna().sum()

data.describe()

corr = data.corr()
corr

def check_correlation(feature_df, target_df):
  r = feature_df.corr(target_df)
  print(f"The correlation coefficient is: {r}.")

feature = "longitude"
target = "lotSizeSquareFeet"
check_correlation(data[feature], data[target])

data["heating"].value_counts()