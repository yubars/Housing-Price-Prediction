# -*- coding: utf-8 -*-
"""Step 1 Data Filtration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TK17YAXOglrKUQBtNJaT2gZWVf-snxJB
"""

import pandas as pd

# !gdown --id "1RJSUB-Wf4WUg1AXMtW2X2wD9B2SIHYT8"

from google.colab import drive
drive.mount('/content/drive')

orig_data = pd.read_csv("2019_assessments_fips_113135_gwinnett.csv")

data = orig_data.copy()

data.columns

data.head()

dump_df = pd.DataFrame()

# "apn" may also not be the potential feature for the regression model, we put it into the "dump_df"
dump_df["apn"] = data["apn"]
dump_df.head()

# Dropping the column "apn" from "data"
data = data.drop(columns="apn")
data.head()

# Since we have divided the address to city, house and siffix and zip code
# We do not need the full address; Keeping it in dump_df

dump_df["address.full"] = data["address.full"]
dump_df.head()

data = data.drop(columns="address.full")
data.head()

new_data = pd.DataFrame()

print(data["address.city"].unique())

# We need the city. We keep the column and convert all the values into lower case
# We add them to new_data
data["address.city"] = data["address.city"]
# Whats the point of the line above this comment? - Joed
new_data["city"] = data["address.city"]
new_data.head()

data = data.drop(columns="address.city")
data.head()

# These are required: address.state,	address.house,	address.street, address.streetSuffix,	address.unit,	address.zip,	address.zip4
# Adding them to new_data
temp_df = data.iloc[:,:7]
temp_cols = list(temp_df.columns)
print(temp_cols)
new_cols = list(map(lambda x: x.split('.')[1], temp_cols))
temp_cols_dict = dict(zip(temp_cols, new_cols))
print(temp_cols_dict)

# We keep the State, House Number, Street Name, suffix, unit and zip code
# State is only one, but can be anything -> We might want to encode all the 50 state of USA
temp_df = temp_df.rename(columns=temp_cols_dict)
new_data = pd.concat([new_data, temp_df], axis=1)
new_data.head()

data = data.drop(columns=temp_cols)
data.head()

# We don't need "fips" since it has only single value, adding to dump_df
print(data["fips"].unique())

dump_df["fips"] = data["fips"]
dump_df.head()

data = data.drop(columns="fips")
data.head()

print(data["county"].unique())

#Only "Gwinnett County" -> Can be dropped
dump_df["county"] = data["county"]
dump_df.head()

data = data.drop(columns="county")
data.head()

print(data["state"].unique())

# Already loaded
dump_df["state"] = data["state"]
dump_df.head()

data = data.drop(columns="state")
data.head()

# We keep latitude, longitude
temp_df = data.iloc[:,:2]
temp_cols = list(temp_df.columns)
new_cols = ["latitude","longitude"]
temp_cols_dict = dict(zip(temp_cols, new_cols))
print(temp_cols_dict)

temp_df = temp_df.rename(columns=temp_cols_dict)
new_data = pd.concat([new_data, temp_df], axis=1)
new_data.head()

data = data.drop(columns=temp_cols)
data.head()

print(data["censusTract"].unique())

# What is censusTract? Do we need it? Lets keep it in confusion_df and remove from "data"
confusion_df = pd.DataFrame()
confusion_df["censusTract"] = data["censusTract"]
confusion_df.head()

data = data.drop(columns="censusTract")
data.head()

print(data["taxID"].unique())

# We don't have taxID, since no value, so add to dump_df
dump_df["taxID"] = data["taxID"]
dump_df.head()

data = data.drop(columns=["taxID"])
data.head()

# How are we going to process the taxAmount and taxYear, are they the direct features for Regression Model? For now lets keep in confusion_df
confusion_df = pd.concat([confusion_df, data.iloc[:,:2]], axis=1)
confusion_df.head()

data = data.drop(columns=["taxAmount", "taxYear"])
data.head()

# What is totalValue? For now lets keep it in "confusion_df"
confusion_df["totalValue"] = data["totalValue"]
confusion_df.head()

data.drop(columns="totalValue", inplace=True)
data.head()

print(data.marketValueYear.unique())

# Since there is only 2019, we may not include this, but still the data could be of any years, so lets add it to confusion_df
confusion_df["marketValueYear"] = data["marketValueYear"]
confusion_df.head()

data.drop(columns="marketValueYear", inplace=True)
data.head()

# Now since the marketTotalValue is the addition of marketLandValue and marketImprovementValue, we can just keep the marketTotalValue,
# which is the label to be predicted by our regresison model

housePrice = data.marketTotalValue.rename("housePrice")
print(housePrice)

# We keep the marketLandValue and marketImprovementValue into the confusion_df, and remove from data
confusion_df["marketLandValue"] = data["marketLandValue"]
confusion_df["marketImprovementValue"] = data["marketImprovementValue"]
confusion_df.head()

data = data.drop(columns=["marketLandValue", "marketImprovementValue", "marketTotalValue"])
data.head()

print(data.landUseGeneral.unique())
print(data.landUseCode.unique())

# What is the meaning of landUseGeneral and landUseCode? Lets add them to the "confusion_df" for now

confusion_df = pd.concat([confusion_df, data.iloc[:,0:2]], axis=1)
confusion_df.head()

data.drop(columns=["landUseGeneral", "landUseCode"], inplace=True)
data.head()

print(data["landUseDescription"].unique())

# We keep the landUseDescription in the new_data
new_data["landUseDescription"] = data.landUseDescription
new_data.head()

data.drop(columns=["landUseDescription"], inplace=True)
data.head()

print(data["zoningDescription"].unique())

# We keep the zoningDescription in the new_data
new_data["zoningDescription"] = data.zoningDescription
new_data.head()

data.drop(columns=["zoningDescription"], inplace=True)
data.head()

# We keep the lotSizeAcres, lotSizeSquareFeet	and lotTopography in the new_data
new_data = pd.concat([new_data, data.iloc[:, :3]], axis=1)
new_data.head()

data = data.drop(columns=["lotSizeAcres", "lotSizeSquareFeet", "lotTopography"])
data.head()

print(data.numberOfBuildings.unique())

# Which number is this talking about? Buildings in a neighbourhood or building owned by the apartment? Should we separate houses and buildings?
# For now, lets keep it in confusion_df
confusion_df["numberOfBuildings"] = data.numberOfBuildings
confusion_df.head()

data.drop(columns="numberOfBuildings", inplace=True)
data.head()

# We don't need url and id, so we add them to dump_df
dump_df["url"] = data["url"]
dump_df["id"] = data["id"]
dump_df.head()

data.drop(columns=["url", "id"], inplace=True)
data.head()

print(data["building.noOfUnits"].unique())

# No. of units is confusing, so lets keep it in to the confusion_df
confusion_df["noOfUnits"] = data["building.noOfUnits"]
confusion_df.head()

data.drop(columns="building.noOfUnits", inplace=True)
data.head()

# What decides the building quality? A, B, C ???
# Lets add it into the confusion_df
confusion_df["quality"] = data["building.quality"]
confusion_df.head()

data.drop(columns="building.quality", inplace=True)
data.head()

# I think we can incoroporate:
# building condition, architecturalStyle, yearBuild, totalStories, totalRooms, totalbedrooms, baths, heating, ac, foundation, fireplace, water, sewer
# so lets add them to new_data
temp_df = data.iloc[:,:]
temp_cols = list(temp_df.columns)
print(temp_cols)
new_cols = list(map(lambda x: x.split('.')[1], temp_cols))
print(new_cols)
temp_cols_dict = dict(zip(temp_cols, new_cols))
print(temp_cols_dict)

temp_df = temp_df.rename(columns=temp_cols_dict)
new_data = pd.concat([new_data, temp_df], axis=1)
new_data.head()

data.drop(columns=temp_cols, inplace=True)
data.head()

'''We have processed all the individual features, we have 4 dataframes and 1 series:

    # Dataframes
    1. new_data: the collection of important features
    2. confusion_df: the confusing features that need to be discussed
    3. dump_df: the irrelevant features
    4. data: empty data, filtered from the raw data
    
    # Series
    1. housePrice: series of housing price that will be used as label to be predicted
'''

# Nw we concat the housePrice label with the important features "new_data"
filtered_data = pd.concat([new_data, housePrice], axis=1)
filtered_data.head()

import os
if not os.path.exists("/content/Data"):
  os.mkdir("/content/Data")

filtered_data.to_csv("Data/filtered_data.csv", index=False)
confusion_df.to_csv("Data/confusion_data.csv", index=False)
dump_df.to_csv("Data/dump_data.csv", index=False)

from google.colab import files
BASE_DIR = "/content/Data/"
filenames = os.listdir(BASE_DIR)
for filename in filenames:
  file_path = BASE_DIR + filename
  files.download(file_path)

"""# NOTES
The data we have is the data belonging to Gwinnett of Georgia (GA). And the data quality is very poor, which required a significant pre-processing. The data is disoriented, imbalanced, incomplete, unordered, including irrelevant features, categorical and numerical features. Therefore, a lot of pre-processing is done in the context.

We have processed all the individual features independently and finally created the following four dataframes and Series.

**Dataframes**
  1. *new_data*: the collection of important features
  2. *confusion_df*: the confusing features that need to be discussed
  3. *dump_df*: the irrelevant features
  4. *data*: empty data, filtered from the raw data

**Series**
1. *housePrice*: series of housing price that will be used as label to be predicted
"""